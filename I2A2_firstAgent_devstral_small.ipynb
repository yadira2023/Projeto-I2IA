{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvWqTodxCWQxupLcQgJ7n4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grupo274/pre-projeto-i2a2/blob/main/I2A2_firstAgent_devstral_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load API key from file"
      ],
      "metadata": {
        "id": "z3euMGHIyWaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#file_path = 'openrouter_api_key.txt'\n",
        "api_key_path = 'devstral_small_api_key.txt'\n",
        "\n",
        "try:\n",
        "  with open(api_key_path, 'r') as file:\n",
        "    API_KEY = file.read().strip()\n",
        "    print(f\"The file was successfully loaded.\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: The file '{api_key_path}' was not found.\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL8T585gvzpY",
        "outputId": "0b1bb2b4-313a-4a58-fc10-9b46ff39acfb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file was successfully loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions that implement the tools"
      ],
      "metadata": {
        "id": "0ieV9WfmyVkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os, json\n",
        "\n",
        "# Define the functions that implement the tools\n",
        "def unzip_file(zip_path: str):\n",
        "  \"\"\"\n",
        "  Unzips a given file.\n",
        "\n",
        "  Args:\n",
        "    zip_path: The name of the ZIP file to unzip.\n",
        "\n",
        "  Returns:\n",
        "    A list of the names of the extracted files, or an error message.\n",
        "  \"\"\"\n",
        "  if zip_path.endswith('.zip'):\n",
        "    try:\n",
        "      print(f\"Attempting to unzip: {zip_path}\")\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        extracted_files = zip_ref.namelist()\n",
        "        zip_ref.extractall()\n",
        "        print(f\"Successfully unzipped {zip_path}. Extracted files: {extracted_files}\")\n",
        "        return json.dumps({\"extracted_files\": extracted_files})\n",
        "    except zipfile.BadZipFile:\n",
        "      return json.dumps({\"error\": f\"'{zip_path}' is not a valid zip file.\"})\n",
        "    except FileNotFoundError:\n",
        "      return json.dumps({\"error\": f\"The file '{zip_path}' was not found.\"})\n",
        "    except Exception as e:\n",
        "      return json.dumps({\"error\": f\"An error occurred while unzipping '{zip_path}': {str(e)}\"})\n",
        "  else:\n",
        "    return json.dumps({\"error\": f\"'{zip_path}' is not a zip file.\"})\n",
        "\n",
        "\n",
        "def read_csv(file_path: str):\n",
        "    \"\"\"Reads a CSV file and returns its content.\"\"\"\n",
        "    for filename in os.listdir(file_path):\n",
        "      if filename.endswith('.csv'):\n",
        "        csv_path = os.path.join(file_path, filename)\n",
        "        try:\n",
        "          print(f\"Attempting to read CSV: {file_path}\")\n",
        "          with open(csv_path, 'r') as f:\n",
        "            # Read only a sample of the file to avoid overwhelming context\n",
        "            content = f.read(1000) + \"\\n... (truncated)\" # Read first 1000 characters\n",
        "            print(f\"Successfully read CSV: {csv_path}\")\n",
        "            return json.dumps({\"file_content_sample\": content})\n",
        "        except FileNotFoundError:\n",
        "          return json.dumps({\"error\": f\"File '{file_path}' not found.\"})\n",
        "        except Exception as e:\n",
        "          return json.dumps({\"error\": f\"An error occurred while reading '{file_path}': {str(e)}\"})\n",
        "      else:\n",
        "        return json.dumps({\"error\": f\"'{filename}' is not a CSV file.\"})\n",
        "\n",
        "\n",
        "def analyze_data(file_path: str, analysis_type: str, column: str, top_n: int = None):\n",
        "  \"\"\"\n",
        "  Analyzes a dataframe loaded from a CSV file based on the provided analysis type and column.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the CSV file.\n",
        "    analysis_type: The type of analysis to perform (e.g., 'most frequent item', 'total volume', 'average', 'top items').\n",
        "    column: The column to perform the analysis on.\n",
        "    top_n: The number of top items to return for 'top items' analysis (required for 'top items').\n",
        "\n",
        "  Returns:\n",
        "    dict: The result of the analysis or an error message.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    print(f\"Attempting to analyze data from: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if column not in df.columns:\n",
        "      return json.dumps({\n",
        "          \"status\": \"error\",\n",
        "          \"message\": f\"Column '{column}' not found in the data.\",\n",
        "          \"available_colums\": list(df.columns)\n",
        "      })\n",
        "\n",
        "    result = None\n",
        "\n",
        "    if analysis_type == 'most frequent item':\n",
        "      if pd.api.types.is_numeric_dtype(df[column]):\n",
        "         # This check should be more nuanced; mode can be applied to numeric data too.\n",
        "         # Let's keep it as is for now based on original code logic.\n",
        "         pass\n",
        "      result = df[column].mode().tolist()\n",
        "\n",
        "    elif analysis_type == 'total volume':\n",
        "      if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "        return json.dumps({\n",
        "            \"status\": \"error\",\n",
        "            \"message\": f\"Error: 'total volume' analysis requires a numeric column, but column '{column}' is not.\"\n",
        "        })\n",
        "      result = df[column].sum()\n",
        "\n",
        "    elif analysis_type == 'average':\n",
        "      if not pd.api.types.is_numeric_dtype(df[column]):\n",
        "        return json.dumps({\n",
        "                    \"status\": \"error\",\n",
        "                    \"message\": f\"'average' requer coluna num√©rica. A coluna '{column}' n√£o √©.\"\n",
        "                })\n",
        "      result = df[column].mean()\n",
        "\n",
        "    elif analysis_type == 'top items':\n",
        "      if not top_n or top_n <= 0:\n",
        "         return json.dumps({\n",
        "             \"status\": \"error\",\n",
        "             \"message\": \"Voc√™ deve fornecer um valor positivo para 'top_n' para a an√°lise de 'top items'.\"\n",
        "         })\n",
        "\n",
        "      # Find a non-numeric column to group by\n",
        "      column = None\n",
        "      for col in df.columns:\n",
        "        if col != column and not pd.api.types.is_numeric_dtype(df[col]):\n",
        "          group_column = col\n",
        "          break\n",
        "\n",
        "      if group_column is None:\n",
        "        # If no suitable categorical column is found, raise an error\n",
        "        return json.dumps({\n",
        "            \"status\": \"error\",\n",
        "            \"message\": f\"N√£o foi poss√≠vel encontrar uma coluna n√£o num√©rica para agrupar e determinar 'top items' para a coluna '{column}'. 'top items' requer uma coluna de agrupamento categ√≥rica.\"\n",
        "            })\n",
        "\n",
        "      # Perform grouping and get top N\n",
        "      grouped = df.groupby(group_column)[column].sum().nlargest(top_n)\n",
        "      result = grouped.to_dict()\n",
        "\n",
        "    else:\n",
        "      return json.dumps({\n",
        "          \"status\": \"error\",\n",
        "          \"message\": f\"Error: Analysis type '{analysis_type}' is not supported.\",\n",
        "          \"supported_analyses\": ['most frequent item', 'total volume', 'average', 'top items']\n",
        "      })\n",
        "\n",
        "    print(f\"Successfully performed analysis: {analysis_type} on column {column}. Result: {result}\")\n",
        "    return json.dumps({\n",
        "        \"status\": \"success\",\n",
        "        \"analysis_type\": analysis_type,\n",
        "        \"column\": column,\n",
        "        \"result\": result\n",
        "      })\n",
        "\n",
        "  except FileNotFoundError:\n",
        "       return json.dumps({\"status\": \"error\", \"message\": f\"CSV file not found at '{file_path}'.\"})\n",
        "  except Exception as e:\n",
        "    return json.dumps({\n",
        "            \"status\": \"error\",\n",
        "            \"message\": f\"Ocorreu um erro ao analisar os dados de '{file_path}': {str(e)}\"\n",
        "        })\n",
        "\n",
        "# Add more tool functions as needed"
      ],
      "metadata": {
        "id": "uvcPerwswNLr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the tools"
      ],
      "metadata": {
        "id": "5YqfXgnDyd7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_functions = {\n",
        "    \"unzip_file\": unzip_file,\n",
        "    \"read_csv\": read_csv,\n",
        "    \"analyze_data\": analyze_data,\n",
        "    # Add more functions if needed\n",
        "}\n",
        "\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"unzip_file\",\n",
        "            \"description\": \"Extract a zip file and list extracted files.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"zip_path\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Path to the zip file to be unzipped\"\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"zip_path\"]\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"read_csv\",\n",
        "            \"description\": \"Read and return CSV data from a folder.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"file_path\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Path to the CSV file\"\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"file_path\"]\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"analyze_data\",\n",
        "            \"description\": \"Performs descriptive statistical analysis on a CSV file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"file_path\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Path to the CSV file\"\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"file_path\"]\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    # Add more tools if needed\n",
        "]"
      ],
      "metadata": {
        "id": "yJCsl1o3oaMo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User prompt"
      ],
      "metadata": {
        "id": "rX7yAiKEyoel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Considere o arquivo 202401_NFs.zip e descompacte-o. Em seguida, analise o arquivo 202401_NFs_Itens.csv para identificar qual item teve maior volume entregue (em quantidade).\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "Cfrca8XeqwMJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agent"
      ],
      "metadata": {
        "id": "i830i4eRyrYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Connect client\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key = API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Add context about the available file and request tool usage explicitly\n",
        "initial_message_content = f\"\"\"\n",
        "                               Para realizar \"{user_prompt}\", elabore um plano passo a passo simples e execute-o usando as ferramentas dispon√≠veis.\n",
        "                               As ferramentas dispon√≠veis s√£o: {json.dumps(tools)}.\n",
        "                               Este plano deve envolver tarefas individuais que, se executadas corretamente, resultar√£o na resposta correta.\n",
        "                               N√£o adicione etapas desnecess√°rias.\n",
        "                               O resultado da etapa final deve ser a resposta final.\n",
        "                               Sempre explique o que voc√™ vai fazer antes de usar uma ferramenta.\n",
        "                               Preste muita aten√ß√£o aos nomes dos arquivos e par√¢metros das fun√ß√µes.\n",
        "                               Se o prompt mencionar um arquivo zip, comece descompactando-o primeiro.\n",
        "                               Depois de descompactar, se o prompt mencionar um arquivo CSV espec√≠fico para an√°lise, use a ferramenta read_csv para inspecionar o arquivo e depois a ferramenta analyze_data.\n",
        "                               Para a an√°lise de 'top items', voc√™ precisar√° especificar a coluna a ser somada e o n√∫mero de itens 'top_n'. A coluna a ser somada para 'volume entregue (em quantidade)' provavelmente ser√° num√©rica e representa a quantidade. Voc√™ tamb√©m precisar√° identificar a coluna que representa os itens (por exemplo, 'produto').\n",
        "                           \"\"\"\n",
        "\n",
        "# Start AI agent loop\n",
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente que usa ferramentas para processar arquivos ZIP e CSV.\"},\n",
        "  {\"role\": \"user\", \"content\": initial_message_content}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"mistralai/devstral-small:free\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "\n",
        "if response and response.choices:\n",
        "  messages.append(response.choices[0].message)\n",
        "\n",
        "  # Tool-calling loop\n",
        "  while hasattr(response.choices[0].message, \"tool_calls\") and response.choices[0].message.tool_calls:\n",
        "    tool_call = response.choices[0].message.tool_calls[0]\n",
        "    function_name = tool_call.function.name\n",
        "    try:\n",
        "      arguments = json.loads(tool_call.function.arguments)\n",
        "    except json.JSONDecodeError:\n",
        "      error_message = \"Error decoding tool call arguments.\"\n",
        "      result = json.dumps({\"error\": error_message})\n",
        "    else:\n",
        "      if function_name in available_functions:\n",
        "        result = available_functions[function_name](**arguments)\n",
        "      else:\n",
        "        result = json.dumps({\"error\": f\"Function {function_name} not found.\"})\n",
        "\n",
        "    messages.append({\n",
        "        \"role\": \"tool\",\n",
        "        \"tool_call_id\": tool_call.id,\n",
        "        \"name\": function_name,\n",
        "        \"content\": result\n",
        "        })\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"mistralai/devstral-small:free\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\"\n",
        "        )\n",
        "    if response and response.choices:\n",
        "      messages.append(response.choices[0].message)\n",
        "\n",
        "  # Final output\n",
        "  if response.choices and response.choices[0].message:\n",
        "    print(\"\\nResposta final do assistente:\\n\")\n",
        "    print(response.choices[0].message.content)\n",
        "  else:\n",
        "    print(\"\\nN√£o foi poss√≠vel obter uma resposta final do assistente.\")\n",
        "else:\n",
        "  print(\"\\nErro: a resposta inicial do modelo n√£o retornou escolhas v√°lidas.\")"
      ],
      "metadata": {
        "id": "LP7NLqN6oBxg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "1082931d-6496-4a8d-e35d-74eebc61ab92",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to unzip: 202401_NFs.zip\n",
            "Successfully unzipped 202401_NFs.zip. Extracted files: ['202401_NFs_Cabecalho.csv', '202401_NFs_Itens.csv']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749513600000'}, 'provider_name': None}}, 'user_id': 'user_2y6pprnU0b6lrBf6Y3joeFFyUIt'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5213282c2db6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         })\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mistralai/devstral-small:free\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749513600000'}, 'provider_name': None}}, 'user_id': 'user_2y6pprnU0b6lrBf6Y3joeFFyUIt'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perguntas\n",
        "1- Considere o arquivo 202401_NFs_Itens.csv, qual √© o assunto que ele possui?\n",
        "\n",
        "R: O arquivo **202401_NFs_Itens.csv** refere-se a dados relacionados com **itens de notas fiscais** (NFs). Pela nomenclatura, podemos inferir que se trata de informa√ß√µes detalhadas sobre os produtos ou servi√ßos associados a notas fiscais emitidas em **janeiro de 2024**. Isso inclui dados como:\n",
        "\n",
        "1. **C√≥digos e descri√ß√µes de produtos/servi√ßos**  \n",
        "2. **Quantidades**  \n",
        "3. **Valores unit√°rios e totais**  \n",
        "4. **Impostos associados** (ICMS, IPI, etc.)  \n",
        "5. **Chaves de acesso das notas fiscais**  \n",
        "6. **Identificadores de clientes e fornecedores**  \n",
        "\n",
        "O assunto principal √© a **gest√£o fiscal e cont√°bil de transa√ß√µes comerciais**, especificamente vinculado √† estrutura de itens que comp√µem as notas fiscais eletr√¥nicas (NF-e ou NFC-e). Esses dados s√£o cr√≠ticos para:  \n",
        "- Controle de estoque,  \n",
        "- C√°lculo de tributos,  \n",
        "- Auditoria fiscal,  \n",
        "- An√°lise financeira.  \n",
        "\n",
        "üìå **Contexto adicional**:  \n",
        "O formato `.csv` indica que √© uma base estruturada, provavelmente gerada por sistemas ERP (como SAP, Totvs) ou emissoras de notas fiscais (ex.: SEFAZ). O prefixo \"202401\" sugere periodicidade mensal (janeiro/2024), comum em relat√≥rios fiscais e arquivos de SPED (Sistema P√∫blico de Escritura√ß√£o Digital).\n",
        "\n",
        "\n",
        "\n",
        "2- Considere o arquivo 202401_NFs_Itens.csv, qual √© o fornecedor que teve maior montante recebido?\n",
        "\n",
        "R:\n",
        "\n",
        "\n",
        "3- Qual item teve maior volume entregue (em quantidade)?\n",
        "\n",
        "R: Para identificar o item com maior volume entregue em quantidade no arquivo **202401_NFs_Itens.csv**, siga os passos abaixo:\n",
        "\n",
        "### Passo a Passo:\n",
        "1. **Leia o arquivo CSV**:  \n",
        "   Carregue o conte√∫do do arquivo usando uma ferramenta que permita manipula√ß√£o de dados (ex: Python, Excel, R).\n",
        "\n",
        "2. **Agrupe os itens por c√≥digo e some as quantidades**:  \n",
        "   - Agrupe todas as linhas pelo campo `CodItem`.  \n",
        "   - Some os valores da coluna `Quant` (quantidade entregue) para cada grupo.\n",
        "\n",
        "3. **Identifique o item com a maior soma**:  \n",
        "   Encontre o registro com o valor m√°ximo na soma das quantidades.\n",
        "\n",
        "### Solu√ß√£o (usando Python com pandas):\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar o arquivo CSV\n",
        "df = pd.read_csv(\"202401_NFs_Itens.csv\", delimiter=';')  # Verifique o delimitador\n",
        "\n",
        "# Agrupar por 'CodItem' e somar as quantidades\n",
        "volume_por_item = df.groupby(\"CodItem\")[\"Quant\"].sum().reset_index()\n",
        "\n",
        "# Encontrar o item com maior volume\n",
        "item_maior_volume = volume_por_item.loc[volume_por_item[\"Quant\"].idxmax()]\n",
        "\n",
        "print(\"Item com maior volume entregue:\")\n",
        "print(f\"C√≥digo: {item_maior_volume['CodItem']}\")\n",
        "print(f\"Total Entregue: {item_maior_volume['Quant']}\")\n",
        "```\n",
        "\n",
        "### Explica√ß√£o:\n",
        "- **groupby(\"CodItem\")**: Agrupa as linhas pelo c√≥digo do item.\n",
        "- **[\"Quant\"].sum()**: Soma a coluna `Quant` (quantidade) para cada grupo.\n",
        "- **idxmax()**: Localiza o √≠ndice do valor m√°ximo na coluna `Quant`.\n",
        "\n",
        "### Resultado:\n",
        "O c√≥digo acima ir√° gerar uma sa√≠da similar a:\n",
        "```\n",
        "Item com maior volume entregue:\n",
        "C√≥digo: [C√≥digo do Item]\n",
        "Total Entregue: [Valor M√°ximo]\n",
        "```\n",
        "Substitua `[C√≥digo do Item]` e `[Valor M√°ximo]` pelo resultado real encontrado no arquivo.\n",
        "\n",
        "### Observa√ß√£o importante:\n",
        "Verifique se o arquivo utiliza **virgula**, **ponto e v√≠rgula** ou outro delimitador. Ajuste o par√¢metro `delimiter` em `pd.read_csv()` conforme necess√°rio. Exemplo comum em dados brasileiros: `delimiter=';'`.\n",
        "\n",
        "Para confirmar o resultado manualmente, voc√™ tamb√©m pode:\n",
        "1. Abrir o CSV em Excel.\n",
        "2. Inserir uma tabela din√¢mica:\n",
        "   - Linhas: `CodItem`\n",
        "   - Valores: Soma de `Quant`\n",
        "3. Ordenar a coluna de soma em ordem decrescente.\n",
        "\n",
        "\n",
        "4- Qual √© o maior consumidor final e o n√∫mero de itens comprados?\n",
        "\n",
        "R: Para determinar o maior consumidor final (pessoa f√≠sica que compra para uso pr√≥prio) e o n√∫mero total de itens comprados, siga as etapas no c√≥digo Python abaixo. O arquivo CSV `202401_NFs_Itens.csv` deve conter uma coluna que identifique o tipo de cliente (ex.: `Tipo_Cliente` com valor `'Consumidor Final'`), um identificador √∫nico para o cliente (ex.: `CPF`) e a quantidade de itens por registro.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar o arquivo CSV (ajuste sep/encoding conforme necess√°rio)\n",
        "df = pd.read_csv('202401_NFs_Itens.csv', sep=';', encoding='latin1')\n",
        "\n",
        "# Verificar colunas (para depura√ß√£o)\n",
        "# print(df.columns.tolist())\n",
        "\n",
        "# Pressupostos sobre as colunas:\n",
        "#   - Coluna de tipo de cliente: 'Tipo_Cliente'\n",
        "#   - Coluna de identifica√ß√£o do cliente: 'CPF'\n",
        "#   - Coluna de quantidade: 'Quantidade'\n",
        "\n",
        "# Filtrar apenas consumidores finais\n",
        "df_consumidor_final = df[df['Tipo_Cliente'] == 'Consumidor Final']\n",
        "\n",
        "# Agrupar por CPF e somar as quantidades\n",
        "compras_por_cliente = df_consumidor_final.groupby('CPF')['Quantidade'].sum()\n",
        "\n",
        "# Identificar o maior consumidor\n",
        "maior_consumidor_id = compras_por_cliente.idxmax()\n",
        "total_itens = compras_por_cliente.max()\n",
        "\n",
        "print(f\"Maior consumidor final: CPF {maior_consumidor_id}\")\n",
        "print(f\"Total de itens comprados: {total_itens}\")\n",
        "```\n",
        "\n",
        "### Explica√ß√£o:\n",
        "1. **Filtragem de Consumidores Finais**: Apenas registros marcados como `'Consumidor Final'` s√£o considerados.\n",
        "2. **Agrega√ß√£o**: Itens s√£o somados por cliente (usando `CPF` como identificador).\n",
        "3. **Identifica√ß√£o do Maior Consumidor**: O cliente com a maior soma de itens √© extra√≠do.\n",
        "\n",
        "### Observa√ß√µes:\n",
        "- **Colunas do arquivo**: Se as colunas no CSV tiverem nomes diferentes (ex.: `CNPJ_CPF`, `Qtde`), ajuste os nomes no c√≥digo.\n",
        "- **Formato do arquivo**: Se o CSV usar separadores diferentes (ex.: v√≠rgula), ajuste `sep` (ex.: `sep=','`).\n",
        "- **Resultado**: A sa√≠da ser√° o CPF do maior consumidor e o total de itens comprados por ele. Se houver empate, retornar√° o primeiro cliente com a quantidade m√°xima.\n",
        "\n",
        "### Execu√ß√£o:\n",
        "Execute o c√≥digo com o arquivo no mesmo diret√≥rio. Se precisar de detalhes da estrutura do arquivo, descomente `print(df.columns.tolist())`.\n",
        "\n",
        "Resultado esperado (exemplo):\n",
        "```\n",
        "Maior consumidor final: CPF 123.456.789-00\n",
        "Total de itens comprados: 150\n",
        "```"
      ],
      "metadata": {
        "id": "y8WCVoIWpmly"
      }
    }
  ]
}